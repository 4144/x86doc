<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8"/>
	<link rel="stylesheet" type="text/css" href="style.css"/>
	<title>PADDB/PADDW/PADDD—Add Packed Integers</title>
</head>
<body>
<h1 id="paddb-paddw-paddd-add-packed-integers">PADDB/PADDW/PADDD—Add Packed Integers</h1>
<table>
<tr>
	<td>Opcode/Instruction</td>
	<td>Op/En</td>
	<td>64/32 bit Mode Support</td>
	<td>CPUID Feature Flag</td>
	<td>Description</td>
</tr>
<tr>
	<td>0F FC /<em>r</em><sup>1</sup> PADDB <em>mm, mm/m64</em></td>
	<td>RM</td>
	<td>V/V</td>
	<td>MMX</td>
	<td>Add packed byte integers from <em>mm/m64</em> and <em>mm</em>.</td>
</tr>
<tr>
	<td>66 0F FC /<em>r</em> PADDB<em> xmm1, xmm2/m128</em></td>
	<td>RM</td>
	<td>V/V</td>
	<td>SSE2</td>
	<td>Add packed byte integers from<em> xmm2/m128</em> and <em>xmm1</em>.</td>
</tr>
<tr>
	<td>0F FD /<em>r</em><sup>1</sup> PADDW <em>mm, mm/m64</em></td>
	<td>RM</td>
	<td>V/V</td>
	<td>MMX</td>
	<td>Add packed word integers from <em>mm/m64</em> and <em>mm</em>.</td>
</tr>
<tr>
	<td>66 0F FD /<em>r</em> PADDW <em>xmm1, xmm2/m128</em></td>
	<td>RM</td>
	<td>V/V</td>
	<td>SSE2</td>
	<td>Add packed word integers from<em> xmm2/m128</em> and <em>xmm1</em>.</td>
</tr>
<tr>
	<td>0F FE /<em>r</em><sup>1</sup> PADDD <em>mm, mm/m64</em></td>
	<td>RM</td>
	<td>V/V</td>
	<td>MMX</td>
	<td>Add packed doubleword integers from <em>mm/m64</em> and <em>mm</em>.</td>
</tr>
<tr>
	<td>66 0F FE /<em>r</em> PADDD <em>xmm1, xmm2/m128</em></td>
	<td>RM</td>
	<td>V/V</td>
	<td>SSE2</td>
	<td>Add packed doubleword integers from <em>xmm2/m128</em> and <em>xmm1</em>.</td>
</tr>
<tr>
	<td>VEX.NDS.128.66.0F.WIG FC /r VPADDB <em>xmm1, xmm2, xmm3/m128</em></td>
	<td>RVM</td>
	<td>V/V</td>
	<td>AVX</td>
	<td>Add packed byte integers from <em>xmm3/m128</em> and <em>xmm2</em>.</td>
</tr>
<tr>
	<td>VEX.NDS.128.66.0F.WIG FD /r VPADDW <em>xmm1, xmm2, xmm3/m128</em></td>
	<td>RVM</td>
	<td>V/V</td>
	<td>AVX</td>
	<td>Add packed word integers from <em>xmm3/m128</em> and <em>xmm2</em>.</td>
</tr>
<tr>
	<td>VEX.NDS.128.66.0F.WIG FE /r VPADDD <em>xmm1, xmm2, xmm3/m128</em></td>
	<td>RVM</td>
	<td>V/V</td>
	<td>AVX</td>
	<td>Add packed doubleword integers from <em>xmm3/m128</em> and <em>xmm2</em>.</td>
</tr>
<tr>
	<td>VEX.NDS.256.66.0F.WIG FC /r VPADDB<em> ymm1, ymm2, ymm3/m256</em></td>
	<td>RVM</td>
	<td>V/V</td>
	<td>AVX2</td>
	<td>Add packed byte integers from<em> ymm2, and</em> <em>ymm3/m256</em> and store in <em>ymm1.</em></td>
</tr>
<tr>
	<td>VEX.NDS.256.66.0F.WIG FD /r VPADDW<em> ymm1, ymm2, ymm3/m256</em></td>
	<td>RVM</td>
	<td>V/V</td>
	<td>AVX2</td>
	<td>Add packed word integers from<em> ymm2,</em> <em>ymm3/m256 </em>and <em>store in ymm1.</em></td>
</tr>
<tr>
	<td>VEX.NDS.256.66.0F.WIG FE /r VPADDD<em> ymm1, ymm2, ymm3/m256</em></td>
	<td>RVM</td>
	<td>V/V</td>
	<td>AVX2</td>
	<td>Add packed doubleword integers from<em> ymm2,</em> <em>ymm3/m256 </em>and <em>store in ymm1.</em></td>
</tr>
</table>
<p class="notes">Notes: 1. See note in Section 2.4, “Instruction Exception Specification” in the <em>Intel® 64 and IA-32 Architectures Software Developer’s Manual,</em> <em>Volume 2A</em> and Section 22.25.3, “Exception Conditions of Legacy SIMD Instructions Operating on MMX Registers” in the <em>Intel® 64 and</em> <em>IA-32 Architectures Software Developer’s Manual, Volume 3A</em>.</p>
<h2 id="instruction-operand-encoding">Instruction Operand Encoding</h2>
<table>
<tr>
	<td>Op/En</td>
	<td>Operand 1</td>
	<td>Operand 2</td>
	<td>Operand 3</td>
	<td>Operand 4</td>
</tr>
<tr>
	<td>RM</td>
	<td>ModRM:reg (r, w)</td>
	<td>ModRM:r/m (r)</td>
	<td>NA</td>
	<td>NA</td>
</tr>
<tr>
	<td>RVM</td>
	<td>ModRM:reg (w)</td>
	<td>VEX.vvvv (r)</td>
	<td>ModRM:r/m (r)</td>
	<td>NA</td>
</tr>
</table>
<h2 id="description">Description</h2>
<p>Performs a SIMD add of the packed integers from the source operand (second operand) and the destination operand (first operand), and stores the packed integer results in the destination operand. See Figure 9-4 in the <em>Intel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 1</em>, for an illustration of a SIMD operation. Overflow is handled with wraparound, as described in the following paragraphs. Adds the packed byte, word, doubleword, or quadword integers in the first source operand to the second source operand and stores the result in the destination operand. When a result is too large to be represented in the</p>
<p>8/16/32 integer (overflow), the result is wrapped around and the low bits are written to the destination element (that is, the carry is ignored).</p>
<p>Note that these instructions can operate on either unsigned or signed (two’s complement notation) integers; however, it does not set bits in the EFLAGS register to indicate overflow and/or a carry. To prevent undetected overflow conditions, software must control the ranges of the values operated on.</p>
<p>These instructions can operate on either 64-bit, 128-bit or 256-bit operands. When operating on 64-bit operands, the destination operand must be an MMX technology register and the source operand can be either an MMX technology register or a 64-bit memory location. In 64-bit mode, using a REX prefix in the form of REX.R permits this instruction to access additional registers (XMM8-XMM15). 128-bit Legacy SSE version: The first source operand is an XMM register. The second operand can be an XMM register or a 128-bit memory location. The destination is not distinct from the first source XMM register and the upper bits (VLMAX-1:128) of the corresponding YMM register destination are unmodified. VEX.128 encoded version: The first source operand is an XMM register. The second source operand is an XMM register or 128-bit memory location. The destination operand is an XMM register. The upper bits (VLMAX-1:128) of the corresponding YMM register destination are zeroed.</p>
<p>VEX.256 encoded version: The first source operand is a YMM register. The second source operand is a YMM register or a 256-bit memory location. The destination operand is a YMM register.</p>
<p>Note: VEX.L must be 0, otherwise the instruction will #UD.</p>
<h2 id="operation">Operation</h2>
<pre>PADDB (with 64-bit operands)
  DEST[7:0] ← DEST[7:0] + SRC[7:0];
  (* Repeat add operation for 2nd through 7th byte *)
  DEST[63:56] ← DEST[63:56] + SRC[63:56];
PADDB (with 128-bit operands)
  DEST[7:0] ← DEST[7:0] + SRC[7:0];
  (* Repeat add operation for 2nd through 14th byte *)
  DEST[127:120] ← DEST[111:120] + SRC[127:120];
VPADDB (VEX.128 encoded version)
  DEST[7:0] ← SRC1[7:0]+SRC2[7:0]
  DEST[15:8] ← SRC1[15:8]+SRC2[15:8]
  DEST[23:16] ← SRC1[23:16]+SRC2[23:16]
  DEST[31:24] ← SRC1[31:24]+SRC2[31:24]
  DEST[39:32] ← SRC1[39:32]+SRC2[39:32]
  DEST[47:40] ← SRC1[47:40]+SRC2[47:40]
  DEST[55:48] ← SRC1[55:48]+SRC2[55:48]
  DEST[63:56] ← SRC1[63:56]+SRC2[63:56]
  DEST[71:64] ← SRC1[71:64]+SRC2[71:64]
  DEST[79:72] ← SRC1[79:72]+SRC2[79:72]
  DEST[87:80] ← SRC1[87:80]+SRC2[87:80]
  DEST[95:88] ← SRC1[95:88]+SRC2[95:88]
  DEST[103:96] ← SRC1[103:96]+SRC2[103:96]
  DEST[111:104] ← SRC1[111:104]+SRC2[111:104]
  DEST[119:112] ← SRC1[119:112]+SRC2[119:112]
  DEST[127:120] ← SRC1[127:120]+SRC2[127:120]
  DEST[VLMAX-1:128] ← 0
VPADDB (VEX.256 encoded instruction)
  DEST[7:0]← SRC1[7:0] + SRC2[7:0];
  (* Repeat add operation for 2nd through 31th byte *)
  DEST[255:248]← SRC1[255:248] + SRC2[255:248];
PADDW (with 64-bit operands)
  DEST[15:0] ← DEST[15:0] + SRC[15:0];
  (* Repeat add operation for 2nd and 3th word *)
  DEST[63:48] ← DEST[63:48] + SRC[63:48];
PADDW (with 128-bit operands)
  DEST[15:0]
  (* Repeat add operation for 2nd through 7th word *)
  DEST[127:112] ← DEST[127:112] + SRC[127:112];
VPADDW (VEX.128 encoded version)
  DEST[15:0] ← SRC1[15:0]+SRC2[15:0]
  DEST[31:16] ← SRC1[31:16]+SRC2[31:16]
  DEST[47:32] ← SRC1[47:32]+SRC2[47:32]
  DEST[63:48] ← SRC1[63:48]+SRC2[63:48]
  DEST[79:64] ← SRC1[79:64]+SRC2[79:64]
  DEST[95:80] ← SRC1[95:80]+SRC2[95:80]
  DEST[111:96] ← SRC1[111:96]+SRC2[111:96]
  DEST[127:112] ← SRC1[127:112]+SRC2[127:112]
  DEST[VLMAX-1:128] ← 0
VPADDW (VEX.256 encoded instruction)
  DEST[15:0] ← SRC1[15:0] + SRC2[15:0];
  (* Repeat add operation for 2nd through 15th word *)
  DEST[255:240]← SRC1[255:240] + SRC2[255:240];
PADDD (with 64-bit operands)
  DEST[31:0] ← DEST[31:0] + SRC[31:0];
  DEST[63:32] ← DEST[63:32] + SRC[63:32];
PADDD (with 128-bit operands)
  DEST[31:0] ← DEST[31:0]
  (* Repeat add operation for 2nd and 3th doubleword *)
  DEST[127:96] ← DEST[127:96] + SRC[127:96];
VPADDD (VEX.128 encoded version)
  DEST[31:0] ← SRC1[31:0]+SRC2[31:0]
  DEST[63:32] ← SRC1[63:32]+SRC2[63:32]
  DEST[95:64] ← SRC1[95:64]+SRC2[95:64]
  DEST[127:96] ← SRC1[127:96]+SRC2[127:96]
  DEST[VLMAX-1:128] ← 0
VPADDD (VEX.256 encoded instruction)
  DEST[31:0]← SRC1[31:0]
  (* Repeat add operation for 2nd and 7th doubleword *)
  DEST[255:224] ← SRC1[255:224] + SRC2[255:224];
</pre>
<h2 id="intel-c-c-compiler-intrinsic-equivalents">Intel C/C++ Compiler Intrinsic Equivalents</h2>
<table>
<tr>
	<td>PADDB:</td>
	<td>__m64 _mm_add_pi8(__m64 m1, __m64 m2)</td>
</tr>
<tr>
	<td>(V)PADDB:</td>
	<td>__m128i _mm_add_epi8 (__m128ia,__m128ib )</td>
</tr>
<tr>
	<td>VPADDB:</td>
	<td>__m256i _mm256_add_epi8 (__m256ia,__m256i b )</td>
</tr>
<tr>
	<td>PADDW:</td>
	<td>__m64 _mm_add_pi16(__m64 m1, __m64 m2)</td>
</tr>
<tr>
	<td>(V)PADDW:</td>
	<td>__m128i _mm_add_epi16 ( __m128i a, __m128i b)</td>
</tr>
<tr>
	<td>VPADDW:</td>
	<td>__m256i _mm256_add_epi16 ( __m256i a, __m256i b)</td>
</tr>
<tr>
	<td>PADDD:</td>
	<td>__m64 _mm_add_pi32(__m64 m1, __m64 m2)</td>
</tr>
<tr>
	<td>(V)PADDD:</td>
	<td>__m128i _mm_add_epi32 ( __m128i a, __m128i b)</td>
</tr>
<tr>
	<td>VPADDD:</td>
	<td>__m256i _mm256_add_epi32 ( __m256i a, __m256i b)</td>
</tr>
</table>
<h2 id="flags-affected">Flags Affected</h2>
<p>None.</p>
<h2 id="simd-floating-point-exceptions">SIMD Floating-Point Exceptions</h2>
<p>None.</p>
<h2 id="other-exceptions">Other Exceptions</h2>
<p>See Exceptions Type 4; additionally</p>
<table>
<tr>
	<td>#UD</td>
	<td>If VEX.L = 1.</td>
</tr>
</table>
</body>
</html>
